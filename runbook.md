# Blue/Green Deployment Operations Runbook

## Overview

This runbook provides operational guidance for responding to alerts generated by the Blue/Green deployment monitoring system. All alerts are sent to the configured Slack channel via webhook.

## Startup Alert

When the alert watcher starts or restarts, you'll receive a startup notification:

**Alert Message:**
```
:information_source: - Log Watcher Started

‚Ä¢ Monitoring: Docker logs from devops-nginx
‚Ä¢ Error Threshold: 2.0%
‚Ä¢ Window Size: 200 requests
‚Ä¢ Alert Cooldown: 300s

Time: 2025-10-30 15:18:58
```

This confirms the monitoring system is active and shows the current configuration.

## Alert Types & Response Actions

### 1. ‚ö†Ô∏è Failover Detected

**Alert Message:**
```
:warning: - Failover Detected

Pool switch detected - traffic is now being served by the backup pool.

- Previous Pool: blue
- New Pool: green
- Total Requests: 1234
- Failover Count: 1

Action Required: Check the health of the blue pool.

Time: 2025-10-30 15:20:15
```

**What It Means:**
Nginx has detected that the primary pool (e.g., Blue) is unhealthy and has automatically failed over to the backup pool (e.g., Green). This is expected behavior during failures or chaos testing.

**Operator Actions:**

1. **Verify the failover is legitimate:**
   ```bash
   # Check which pool is currently serving traffic
   curl -i http://localhost:8080/version
   # Look for X-App-Pool header
   ```

2. **Investigate the failed pool:**
   ```bash
   # Check logs of the failed pool
   docker compose logs app_blue

   # Check container health
   docker compose ps

   # Test the failed pool directly
   curl -i http://localhost:8081/healthz
   ```

3. **Check if chaos mode was triggered:**
   ```bash
   # Check if chaos mode is active
   curl http://localhost:8081/version
   # Look for chaos indicators in response
   ```

4. **If unintentional failure:**
   - Review application logs for errors
   - Check resource utilization (CPU, memory)
   - Verify external dependencies are accessible
   - Consider restarting the failed container if necessary

5. **Recovery:**
   ```bash
   # Stop chaos mode if active
   curl -X POST http://localhost:8081/chaos/stop

   # Wait for fail_timeout (5s) + health check recovery
   # Monitor logs to confirm primary pool recovers
   docker compose logs -f nginx
   ```

**Expected Outcome:**
- Backup pool serves traffic successfully
- Zero failed client requests
- Primary pool recovers within 10-30 seconds after issue resolved

---

### 2. üö® High Error Rate Detected

**Alert Message:**
```
:rotating_light: - High Error Rate Detected

Upstream error rate has exceeded the threshold.

- Current Error Rate: 5.50%
- Threshold: 2%
- Errors in Window: 11/200
- Current Pool: blue
- Total Requests: 2500

Action Required: Investigate upstream logs and consider toggling pools.

Time: 2025-10-30 15:22:30
```

**What It Means:**
The upstream application is returning 5xx errors at a rate exceeding the configured threshold (default 2%). This indicates a problem with the application itself, not just a single container failure.

**Operator Actions:**

1. **Assess the severity:**
   ```bash
   # Check current error rate in real-time
   curl -i http://localhost:8080/version

   # Make multiple requests to confirm
   for i in {1..10}; do
     curl -s -o /dev/null -w "%{http_code}\n" http://localhost:8080/version
   done
   ```

2. **Investigate the current pool:**
   ```bash
   # Check logs for errors
   docker compose logs --tail=100 app_blue

   # Look for patterns:
   # - Database connection errors
   # - Timeout errors
   # - Application exceptions
   # - Resource exhaustion
   ```

3. **Check the backup pool health:**
   ```bash
   # Test backup pool directly
   curl -i http://localhost:8082/healthz
   curl -i http://localhost:8082/version
   ```

4. **If backup pool is healthy, consider manual failover:**
   ```bash
   # Update .env to switch active pool
   # Change ACTIVE_POOL=blue to ACTIVE_POOL=green

   # Restart services to apply change
   docker compose down
   docker compose up -d
   ```

5. **Root cause investigation:**
   - Check application metrics (if available)
   - Review recent deployments or configuration changes
   - Verify database connectivity and performance
   - Check external API dependencies
   - Review resource utilization

**Expected Outcome:**
- Error rate returns to <2%
- Root cause identified and resolved
- May require application code fix or infrastructure adjustment

---

### 3. üü¢ Pool Recovery Detected

There are two types of recovery alerts:

#### 3a. Recovery from Failover

**Alert Message:**
```
:white_check_mark: - Pool Recovery Detected

Traffic has recovered back to the primary pool.

- Previous Pool: green
- Current Pool: blue
- Total Requests: 3456
- Failover Count: 2

Time: 2025-10-30 15:25:10
```

**What It Means:**
The primary pool has recovered and Nginx has automatically switched traffic back from the backup pool. This is the expected outcome after a single pool failure.

#### 3b. Recovery from High Error Rate

**Alert Message:**
```
:white_check_mark: - Pool Recovery Detected

System has recovered from high error rate.

- Current Pool: blue
- Current Error Rate: 0.50%
- Threshold: 2%
- Total Requests: 4200
- Failover Count: 2

Time: 2025-10-30 15:28:45
```

**What It Means:**
The system has recovered from a degraded state where error rates exceeded the threshold. This alert triggers when both pools were experiencing high error rates and have now recovered.

**Operator Actions:**

1. **Confirm recovery:**
   ```bash
   # Verify primary pool is serving traffic
   curl -i http://localhost:8080/version
   # Should show X-App-Pool: blue (or your primary pool)
   ```

2. **Verify stability:**
   ```bash
   # Run load test to ensure stability
   for i in {1..50}; do
     curl -s -o /dev/null -w "Request $i: %{http_code}\n" http://localhost:8080/version
     sleep 0.5
   done
   ```

3. **Document the incident:**
   - Record the time of initial failover
   - Record the time of recovery
   - Document root cause if identified
   - Note any manual interventions performed

4. **Post-incident review (if major outage):**
   - Review timeline of events
   - Identify opportunities for improvement
   - Update monitoring thresholds if needed
   - Document lessons learned

**Expected Outcome:**
- System operating normally
- Both pools healthy
- Primary pool serving production traffic

---

## Alert Cooldown & Rate Limiting

**Cooldown Period:** 300 seconds (5 minutes) by default

**Purpose:** Prevent alert spam during repeated or oscillating failures

**Behavior:**
- Each alert type has independent cooldown
- Alerts are suppressed during cooldown period
- Events are still logged but Slack notifications are not sent
- Console logs will show: `[INFO] Failover detected (blue ‚Üí green) but cooldown active`

**Adjusting Cooldown:**
```bash
# Edit .env file
ALERT_COOLDOWN_SEC=600  # 10 minutes

# Restart watcher
docker compose restart alert_watcher
```

---

## Maintenance Mode

**Purpose:** Suppress alerts during planned maintenance or testing

**Enabling Maintenance Mode:**
```bash
# Edit .env file
MAINTENANCE_MODE=true

# Restart watcher
docker compose restart alert_watcher
```

**When to Use:**
- Planned blue/green toggle operations
- Chaos engineering drills
- Load testing
- Deployment windows

**Important:** Remember to disable maintenance mode after maintenance completes:
```bash
# Edit .env file
MAINTENANCE_MODE=false

# Restart watcher
docker compose restart alert_watcher
```

---

## Monitoring & Diagnostics

### View Watcher Logs
```bash
# Real-time watcher logs
docker compose logs -f alert_watcher

# Last 100 lines
docker compose logs --tail=100 alert_watcher
```

### View Nginx Access Logs
```bash
# Access logs with detailed JSON format
docker compose exec nginx tail -f /var/log/nginx/access.log

# Parse and pretty-print
docker compose exec nginx tail -f /var/log/nginx/access.log | jq .
```

### Check Current Pool Status
```bash
# Check which pool is active
curl -i http://localhost:8080/version | grep X-App-Pool

# Check both pools directly
echo "Blue Pool:"
curl -i http://localhost:8081/version | grep -E "(X-App-Pool|X-Release-Id)"

echo "Green Pool:"
curl -i http://localhost:8082/version | grep -E "(X-App-Pool|X-Release-Id)"
```

### Manual Chaos Testing
```bash
# Trigger errors on blue pool
curl -X POST http://localhost:8081/chaos/start?mode=error

# Trigger timeouts on blue pool
curl -X POST http://localhost:8081/chaos/start?mode=timeout

# Stop chaos mode
curl -X POST http://localhost:8081/chaos/stop
```

---

## Troubleshooting

### Alert Watcher Not Sending Alerts

**Symptoms:** No Slack messages despite failover events

**Checks:**
1. Verify SLACK_WEBHOOK_URL is set correctly:
   ```bash
   docker compose exec alert_watcher env | grep SLACK_WEBHOOK_URL
   ```

2. Check watcher logs for errors:
   ```bash
   docker compose logs alert_watcher | grep ERROR
   ```

3. Test Slack webhook manually:
   ```bash
   curl -X POST -H 'Content-type: application/json' \
     --data '{"text":"Test alert from Blue/Green deployment"}' \
     YOUR_SLACK_WEBHOOK_URL
   ```

4. Verify maintenance mode is disabled:
   ```bash
   docker compose exec alert_watcher env | grep MAINTENANCE_MODE
   ```

### False Positive Alerts

**Symptoms:** Alerts triggered during normal operation

**Solutions:**
1. Adjust error rate threshold:
   ```bash
   # Edit .env
   ERROR_RATE_THRESHOLD=5  # Increase from 2% to 5%
   ```

2. Increase window size for more stable measurements:
   ```bash
   # Edit .env
   WINDOW_SIZE=500  # Increase from 200 to 500
   ```

3. Increase cooldown to reduce noise:
   ```bash
   # Edit .env
   ALERT_COOLDOWN_SEC=600  # Increase from 300s to 600s
   ```

### Missing Log Fields

**Symptoms:** Alerts show empty pool or release fields

**Checks:**
1. Verify Nginx log format is correct:
   ```bash
   docker compose exec nginx cat /etc/nginx/conf.d/default.conf | grep log_format
   ```

2. Check that application is setting headers:
   ```bash
   curl -i http://localhost:8081/version | grep -E "(X-App-Pool|X-Release-Id)"
   ```

3. Verify Nginx is capturing upstream headers:
   ```bash
   docker compose exec nginx tail -5 /var/log/nginx/access.log | jq .
   ```

---

## Configuration

### Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `SLACK_WEBHOOK_URL` | (required) | Slack incoming webhook URL for alerts |
| `ACTIVE_POOL` | `blue` | Initial active pool name |
| `ERROR_RATE_THRESHOLD` | `2` | Error rate threshold percentage |
| `WINDOW_SIZE` | `200` | Sliding window size for error calculation |
| `ALERT_COOLDOWN_SEC` | `300` | Cooldown period between alerts (seconds) |
| `MAINTENANCE_MODE` | `false` | Suppress alerts during maintenance |

### Nginx

| Setting | Value | Purpose |
|---------|-------|---------|
| `proxy_connect_timeout` | 2s | Max time to establish upstream connection |
| `proxy_send_timeout` | 2s | Max time to send request to upstream |
| `proxy_read_timeout` | 2s | Max time to read response from upstream |
| `fail_timeout` | 5s | Time server is marked down after failure |
| `max_fails` | 1 | Failures before marking server down |

---

## Contact Information

- **On-Call Engineer:** Check your on-call schedule
- **Slack Channel:** #blue-green-alerts
- **Incident Management:** Your incident management tool/process
- **Documentation:** This runbook

---

## Version History

- **v1.0** - Initial runbook for Blue/Green deployment monitoring
- **Date:** 2025-10-30